version of alana
that talks just about movies but does it really well.

limit what alana has access to now , fix nlu to just understand things relating to movies

use case
Use cases: ---------------------------------------------------------------------------------------------------------------------------
Conversational search “Browsing mode Navigator and discover” 
 
User: “We want to find something to watch tonight” [NEW INTENT: ask-for-recommendation]
Alana: “How about another romantic comedy like yesterday?” [SUGGESTION with MEMORY]
User: “OK, are there any from the 1980s?” [update NLU] → API call
Alana: “Yes how about Splash or When Harry Met Sally?” [NLG + recommendation policy: from Google movies/IMDB?]
User: “Who stars in Splash?” [QA bot / EVI]
Alana: “Splash stars include Tom Hanks, Daryl Hannah, and Eugene Levy.” [already works]
User: “OK I like Tom Hanks. Let’s do it!” [add like to user profile; play Splash]
Alana: “Great. Playing Splash now”
Idea: “I have already seen Splash” → save to user profile
NLU for user language: I want something light; nothing serious; no horror; nothing violent; something for kids; a sci-fi from the 1980s
Experiment with current Wiki-bot e.g. What’s the plot of X?
Later add: repairs “no not 1980s, 1990s” etc
Use the user profile to help with recommendations e.g. we know they like films with Bill Murray, so rank that one higher

Social/in-home, co-searching and negotiating (multi-user interaction) 
 
User 1: “Let’s find something to watch. Can we see a sci-fi film?”
Alana: “Sure we have a lot to choose from that you haven’t seen yet. How about a classic from the 80s, or maybe something more recent?”
User 2: “As long as it’s not horror, I hate horror”
User 1: “ok that’s fine. Are there any classic ones then?”
Alana: “A classic sci-fi film, not horror. OK - what about Back to the Future or 2001 a space odyssey?”
User 2: “what is Back to the Future?
Alana: “Back to the Future is an adventure, comedy, sci-fi film from 1985 starrring Micheal J Fox.”
User 1: sounds good
User 2: Tell me more
Alana: “Here’s what IMDB says: Marty McFly, a 17-year-old high school student, is accidentally sent thirty years into the past in a time-traveling DeLorean invented by his close friend, the eccentric scientist Doc Brown.”
User 2: OK let’s do this!


multi user genre searching
terms used when discussing movies
list of movies , movie titles misspelled, good databases for this ?

retrain nlu , writing regular expression
list of movies kept of movies the user has seen before
web page called rapid api.com, certain amount of free data per month
get data from rapid api
recency of movie , suggest the movie , use the wiki api , or scraping the wiki api , but will need to make a cronjobn to automate the scripting since we will need to make a script
that exports certain things from wiki pedia not the entire wikipedia
indexing this into the database once every /month/few weeks
can we get movie from 1980 could be harder to do
if we scrap everything we wil have to ourselves create a ranking
imdb rapidai?
movie bot taps in to rapidapi
wiki scraper
could just do user profiling , to rerank the  recomendation based off user profile,  which could be a latency problem
pro of having the wiki dump locally is you can manipulate it the way you want you don't have to detect it locally,
mongodb movies update every night , in the other its gonna be an api
urrently just make bot and use offline data,  focuso on dialogue flow
use entity linker 
can do offline update idea
what do you with imdb  what do you with recomendations with imdb
wiki bot just says what the first line of wikipedia says , add ontop off that soemthing fuck what did he say
answer question who is starring , and such
but if you say tell me more have iwkipedia answer that

2 things
 recomendations - what we do not do , use api 
information - what we do
have deeper conversation on the movie - imdb
we;re gonna template hte questions and answers we're not gonna open domain , make extremly constrained
"what trevor said "that not possible how do handle this
ontology bot qeries an end point and uses a template , steal steal steal 
focus on use case 1 at all costs
famiiarise with ontolgy bot - if have questions ask herve

do we want focused scraping from wikipedia -  yes?
"who stars in splash"
send a query to imdb where star is an actor intent and splash is the argument , not sure that works
you would need to test infobot in movie domains
sabrina and yanchao should be able to create an extra endpoint to direct info bot to ask questions about wikipedia
step after that is to just scrape the movies 

Ontology Bot---------------------------------------------------------------------------------------------
imbd utils important i think
so the entity linker , using blink or fell https://github.com/yahoo/FEL? https://gitlab.com/alanaai/alana_basecode/fel_server finds the wikidata link then the blazegraph gets all related entries and ontology handles communication,
what apis lack is linking between the results , so we will want to decide 
if we want to construct api calls based off user profile or we use entity linking
we're not going to subsitute entity linking fully with these , we're trying to use apis
to not have to keep updating wiki dump , retrieving vs linking , wiki links to answer questions
but not recomendation

questions where user profile in alana kept?
nlu intents? topics? sentiment? none is an intent
split across various apis
check coherence bot for user profile but overly simple , need to seperate userprofile in its
own module , what sort of data structures should be using to keep all this information , going to be linked to the lu
could be another bot/ after the nlu / part of the nlu , get the annotations user linkers/annotations / built 
a module
steps just get it to call an api and learn how to process the results 
so i can make an api reader class , can work on now would make things clearer , flexible way
tasks ------------------------------------------------------------------------------------------------------------------------------------------------------------
ontology bot familiarise and rapid pro api explore and familiarise - current main tasks ,keep an eye on use case 1 on an actual product, how would we change the ontology bot
what intents are needed , what things are needed to be handled
given what arash said is it gonna be possible to self correct
make a list of what cna be retrieved with the api calls
what apis am i using what can they do , which ones do i have access to
how do i query an endpoint
first step would to be to figure out a way to tap into api
herve thinks i should build functionality of the bot
ask about memory 
organise these notes
which script i can  handle with apis and which i cannot
create new intents?
what are https://www.mediawiki.org/wiki/Incremental_dumps
able to repair after user corrections - ask for examples from ioannis
the tell me about phrase is used in ontology bot
the intents are super barebones right now
can imitate any intent parameters i want , " i want action movies from the 1980s" ass 1/2 intents as genre and query 
different intent for each of the imdbapi parameter , need 5 different intents 1 for each filter 
write example intents with parameters , new recomendation intents , regular expression but a lot of it is neural classifier
for demo we're doing regular expression stuff

old school superhero = this time
don't lose too much on generation and first bit, just focus api
focusing on design this week , intens , user profile , api , 
genre intent
read wiki bot -, imports into mongodb , already does that?
get databases to be used
recency  should be taken into account
ask how to train nlu 
get data for training nlu
change wikibot script
get genre tag list

marco explanation
-where data
whats been done
- sheets 
- what we are doing
what i've done already

make our own movie database
start making intents with nlu?
trained regex , rassa style
use ontologies, create ontoogy graph? what?
either we create our own graph/otnologys or we use what we have plus api calls
what is template nlg
main logic , entirely new movie bot plus info bot
use mongodb if we're qurying by document
quick literature review
a plot synopsis dataset 
index just about wikidata

scrape api , - main job download 1000 movies , enormous row with all the data in the world, use sql or mongodb , datasets for user queries
have a data storage python class scraper calls that

what attributes does the person object have , 
role movies they'ree in,

 what aspect does the movie objec t have
user query collaborative filtering
x most recent movies priority + some popular tv series
start with much less movies like 100 
how train an entity linker#

disambiquity function owned by the movie bot

blink looks at the context in which a word is mentioned
simple entity linker starting point("aha you said the thing")
TOdoo 
finish scrape
write examples (crash coherence bot example)
thiink of thing ive been dong if yannis
prepare lists the movie bot will accept of diferent categories and such
100 action movies
100 x movies
movies of x era
bad movies
so bad their good movies
indie movies 
problem movies like movie , crash , recomend
temporarily the movie botn eeds to handle lord of the rings as a series like if the uses just say lord of the rings whihc one

wikidata linking
rasa classifiers or relu
need ontology of genres
yannis wants a similiarity recommendation for mvp
make a list of  movie names actor names search for them
recomend the movie , search movie,

need to define 3 

IMPORTANT
python -m movie_bot-main.moviebot.scrapers.apiscraper

todo

scrape more rnib things - done
refactor all scrapers
look for intresting dialogues in last quarter of dialogues
add rank to api scraper
write a similarity function between 2 movie entities? also returns the reason for similarity
get version 0 dialogues and extent intents
user utterances we want for the nlg , save system utteraces that match ones we want from nlp , remove titles and replace with x ,y and z
annotate rapidpro


populate dataset fromopendialkg use to make regex
regular expressions on entititess : genre , decade , movie , person , search , info
collect training data for rasa nlu -primary focus,  make relu for intents as much as can


single intents
make responses that also ask a question when answering a query
ask for clarification if their are too many recomendations
user scripts from alana tv analaysis sabrina made https://docs.google.com/document/d/1lZZUMjZR0WcklrwPZQ7T0TQHuNT-IrgVmYSYq4n66gw/edit?usp=sharing
objective analaysis person ask on genre/actor/date so we can understand what the best suggestions would be

put user modelling tasks on monday - later once sabrian mettin
look into scraping keywords imdb
violence rating
user modelling
watch igor, read papers
get response with dummy profile
where are we store it 
version 0 use the existing state object
ask how to preload userprofile on state object  - sentiment intensityy analyser could be used
task scrape keywords
user dialogues for recomending and as part of the ranker, recomend based on preferencess
implement ranker sub task
recomend user profile

problem with new dataset
duplicates
languages
nonsense
unlabeled

arash funny thing
herbert clarks ground 1996
conversation only moves forward when you agree you understand
meaning is collaborative
levels are attendance, perception ,understanding ,  acceptance 
meta communication is communication about communication

shiv complaints
structure has too many dependencies
you can use terraform now to load entire infrastructure on terra
you need a vpn client to access alana full infastructure now
aws vpn client
all our user data need to be stored on userdb
all communications need to be asyncrhonous to be truly scalable,  rn they are all sychronous
in gitlab in our wiki their is our architecture
need more production frameworks like fast api
what in christ is dependancy injection
whenever downtime , test flask vs fastapi performance difference
ask him to help with how to dockerise stuff how to push it to a pipeline

unicef incoming todo
annotate rapid pro dataset and clean it up
language identification problem
https://www.who.int/emergencies/diseases/novel-coronavirus-2019/advice-for-public/myth-busters
talk to marco before i leave to talk to Marco
think of format we want the data to have ( rumour binary or scalar?) focus on removing health related harmful negatives
research rumour classifier , bert?
multiple classes - clustering, (gossipy rumours , claims about 1 person , health rumours , non sequitors)
yiannis wants this used for clustering https://aclanthology.org/2020.acl-main.692.pdf  soft clustering
afer clusterig human intervention to put certain clusters together
check if cluster agrees with their labelling
fine tune roberta/use it normally , cluster , annotate , don't  say its a non rumour just that it doesnt belong  to othr categoris




https://docs.google.com/spreadsheets/d/1Tn8LsRXo8iJuTUyXuoLibt3kK_Ne9852EUfomwUQ_T8/edit#gid=159188870

additional facts from new website do this first - done
#annotate examples of multiple myths
https://www.unicef.org/coronavirus/navigating-pregnancy-during-coronavirus-disease-covid-19-pandemic
put in multi facts as comma delimited list
update to annotate list send to unicef
then funny shiv
build clusters based on myths and unverified , or just unverified
mess around with clustering , fix data formattings
label prediction
update unferified myth list 2
search for other hyperparameter tuning solutions
further hyperparameter tuning
change classifier to work on new taxonomy of labels(whats the current one?)
fix monday
change classifier to not deal with unerified
present this is the outcome the best number of clusters is 22 , this would produce the most accurate predictor
of myths

i have multilabeled with unverifid and myth i should fix this before presenting the dataset to others

example of nonsense - 
"coronavirus is a man"
make dataset 2.0

i'm doing relabeling of new stuff
we do relabeling in november
for relabelling keep lower number
integration with rapidpro???
bothub
we make microservice for them (whats a microservice)

follow up question after providing answer , who told you this?
response to that , 
table lookup with templates and response generator nothing crazy
where did you hear that, do you believe thats true
do you stil believe that , was the fact useful for changing your mind,
was the question appropiate
negative response for unverified nmyths or wasn't useful here's a link to a pile o myth
make document explaining plan
propose random or linear routing through questions , not always ask certain things



do we want to restart the process at api error or what?

i have had to delete various inputs such as ""Dear U-Reporter, 

Based on your participation in the last NPHCDA poll on COVID-19 vaccine uptake in Nigeria, UNICEF Nigeria will like to nominate you as a "Youth Vaccine Champion"

If you will like to participate in championing the COVID vaccine uptake in your community, please reply with 

A. Yes
B. No"" 
for being too internety and nonsensical


shiv is making my life easier and docker less heavy
kubernetes distribution
1.25
rancher desktop exe on github
push images to rancher desktop which is a kubernetes wrapper
installation
mongodb helm add 
ppa to source install the software
bitnami , huge list of helm applications
noricutl + rancher desktop will remove docker - sounds fuckin good to me
missing docker pull
devspace sh




shiv
project structure
route is security and reciee user
service is where we mess with response
config ,tsores urls and caches

5 GB RAM Peak


i've been hearing that covid cannot survive hot weather
rename item
add assertation in first 2 unit tests that the data is saved correctly


shiv terrifying setup
---------------------------------------------------------------------------------------------------
kubernetes ---------------
goal is to fix disconnect between prod and dev, this disconnect is the cause of "works on my machine"
goal is we should have all the same stuff as prod
kubernetes is thicc , can't have a local machine
rancher is skiny kubernetes, mostly a lot of legacy crap we never use
replaced some core services to be more like k3S (kubernetes slimmed down for other nerds)
you can't test 5 instances of server with docker desktop, with k3s you can spin up 5 instances
without having an emotional and ram  breakdown
not perfectly like prod enviroment due to change in resources but should be close

virtual machine bad for communicating with kafka or running multiple services
work inside kubernetes as if we were running in a virtual enviroment
devspace? will load into my kubernetes distribution and then synchronise the changes
puts your project into kubernetes and it synchronises it into the kubernetes container
so the application is always up to date with local changes , don't need to build images just need to 
run devspace

When run devspace?

steps 
1. bring the github you need onto local machine
2. deploy command in devspace?

problem i don't have enough vram or ram or gpu or  whatever
answer - use telepresence (kubernetes feature) , external service calling a local service is weird
so with telepresence you can run local bots of external service , i don't understand
external service is on the cloud, runs like ngrok where you make local service an ip

there are other stuff (cubectl?) to deploy to local enviroment

need api mocking solution
https://gitlab.com/alanaai/alana-docs/-/wikis/Developer-Environment
---------------------------------------------------------------------------------------------------

stuff 

MongoDB&reg; can be accessed on the following DNS name(s) and ports from within your cluster:

    mongo-mongodb.dev.svc.cluster.local

To connect to your database, create a MongoDB&reg; client container:

    kubectl run --namespace dev mongo-mongodb-client --rm --tty -i --restart='Never' --env="MONGODB_ROOT_PASSWORD=$MONGODB_ROOT_PASSWORD" --image docker.io/bitnami/mongodb:4.4.10-debian-10-r44 --command -- bash

Then, run the following command:
    mongo admin --host "mongo-mongodb"

To connect to your database from outside the cluster execute the following commands:

    kubectl port-forward --namespace dev svc/mongo-mongodb 27017:27017 &
    mongo --host 127.0.0.1

C:\Users\tarta>
i have a github cli now from gh,look up commands for that
https://gitlab.com/alanaai/alana-docs/-/wikis/Developer-Environment
to get an overview of enviroment cd to the octant place in quick access and run the exe from terminal

to make file run go into the devspacestart.sh equiavalent and make it run the thing 
this is where to beg herve for help
service stores everything in cache
and the bot dips into the cache now
when theirs leftover crap , devspace -- reconfigure
this will help give it the old off and on again
devspace.yaml modifying stuff should fix shit

confluence kafka and kafka python exist 

look at alanahub and coherence bot and for quick and dirty alana demo app
kafka partitions?
TODO

prepare demo version
prepare test examples for demo
integrate marcos new models -  lata
make a video of the demo
integrate with my dekstop github with the new github


high confidence  is it true that covid cannot survive in hot areas

low confidence is it true covid dislikes the heat

a topic is a po box in shivs analogy where you don't know who sent it till you read the letter in the topic
alanna hub is a consumer listening to the topic , processes it can send it to a bot request topic
https://github.com/McGill-NLP/topiocqa  -json dataset reader code















 use dataframe to csv function
empathy bot ------------------------------------------
"companion app"
visual dialog on one end
and the  odqa and the conversational nlu capabailities that ara (e.g. persona bot opinion bot)sh is working on
missing chit chat dialogue that we had in social alana
fture task global dialogue management
joining yannis team working with demetris and sabrina
seq2seq?
they want to have infobot back live with new architecture, sparse retriever, running on haystack
haystack has something on the backend that makes it arbitrarily scalable
haystack is the retriever
FID - future , something decoder
question what topic is the question
topic OCQA
read dimitris paper hard working toward they numbers in the BM25 f11 score and DR retirever
what is FiD
task handle user feedback
identifying intents - task
 in unicef data dimitris code picture on how to run a query

either give infobot with bots opinion when infobot responds
or respond with opinion in response to user opinion

would the response for the proactive resonse require the personabot to be integrated with the infobot?
dialogue manager problem who choose infobot

my task
search for emphathtic bot
which papers am i recommending to the others
read OTTers one turn top transitions for open-domain dialogue
acknowledge and continuing , bridging
really need to research FID
test telegram infobot ( get questions from friends)
we're only doing single shot responses , we try to express an opinion whilst getting the user to 
prompt and continue
topioQA FID
papers on writing responses that prompt further questions for the user
how are we making this multishot
read up on dpr
one shot learning vs few shot vs deep learning
organise notes and set times
funny gpt-2 bot 
work on rule based state machine that is easier to maintain
task- play around with some different apps and write a comparison
play around with info bot insce we are integrating with ti
run replication tests in githubs found in order to fix your face
try woebot (DO NOT USE Alana E|MAIL) https://mental.jmir.org/2017/2/e19/PDF
read monday board in q1 yannis comments
read sabrina demetris board 
look at elizab ot ioanis hates it ,  
play around with sympathettic
fsm bot simple bot
research seq2seq
fsb bot
persona chat paper
read old persona bot (does alana like music like pop music)
research OCQA
seq2seq trained on human interactions on x topic and using regular expression and rules to find
agree and disagree options to curate the dataset, need to be able to control whether alana agrees or disagrees
early goal interact with existing systems and reading papers

i'm in charge of the rule based bot i'm on my own 
we meet everyday at 9.30am  for 30 minute huddles
seq2seq task find existing dataset and skullfuck it
make rule based seq2seq opinion bot goal infer an opinion due to topical preference

whenever i can find time try to work on feature C , social aspect of how to generate 
the social aspect of responding in example 2

yannis paper notes-------------------------------------------------------
our context is wikipedia , 
https://arxiv.org/pdf/2107.07566.pdf - decision point, there are problems with static models
do we want to puruse online models https://medium.com/distributed-computing-with-ray/retrieval-augmented-generation-with-huggingface-transformers-and-ray-b09b56161b1e
yannis is more worried about inspration on datasets
first is topic sifting
second is ?
third is ?
look for some more papers on https://aclanthology.org and on semantic scholar for people who 
cited it
 4th has dataset links
topical chat 
https://m.media-amazon.com/images/G/01/amazon.jobs/3079_Paper._CB1565131710_.pdf
11k dataset, 
discussion points 1 user statement 1 response
8 braod categories,  seems very americanised lotsa football
https://github.com/alexa/Topical-Chat/blob/master/conversations/test_freq.json
remembber we are doing question answering
find research papers on modelling "controversy" for long term for now whit elist by hand

good test points in topology
have a topic that alana is not intrested in but have her intrested in a sub topic
we want the me too response i the example in chat to be based on the topology we've made

we express what topics we want to have an opinion on my hand, "popular songs by female singers in the 90s"
 pipeline.run is the querys and that gives you 10 documents , then the fid component comes in and decodes the 
components  and is done by the fid generator, 

important to  read
papers about making responses
https://arxiv.org/pdf/1704.01074.pdf
https://arxiv.org/pdf/1711.04090.pdf

we encode the section title when we retrieve passages into fid - what mean
chatbts
----------------------------------------------------------------
Mitsuku, Cleverbot, XiaoIce, and DialoGPT

test dialog - 
test no i meant x in bots
basic question , 
chit chat
check if its leading

document store contains the database

https://arxiv.org/ftp/arxiv/papers/1801/1801.01957.pdf
xiao uses retriavl based trained on user to user message response pairs  that are indexed by messages that invoke
them what exactly does that mean
also uses generation based.  seq2seq generation/retrieval
demitri linked a relevant paper


https://arxiv.org/abs/2001.09977v3
meena google bot that uses our seq2seq thing with code.
https://paperswithcode.com/paper/towards-a-human-like-open-domain-chatbot#code
https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html

rule based chatbot overview
file:///C:/Users/tarta/AppData/Local/Temp/Adamopoulou-Moussiades2020_Chapter_AnOverviewOfChatbotTechnology.pdf
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3567047



proposal on input/output 

what does it condition on to generate something as an output
plan dive through for the training description and dataset



The dataset used to train Meena is mined and fil-
tered from public domain social media conversa-
tions. The source data are essentially message
trees involving multiple speakers: the very first
message is the root; replies to a message are its
child nodes. Any path along the tree induces a
conversation where each message is a conversa-
tion turn. By treating each turn in a conversation
path as a response and all the previous turns (up to
7) as a context, we create a training example of the
form (context, response) pair.
We also filter the data to improve the genera-
tion quality. A message is removed if any of the
following conditions holds: 1. the number of sub-
words is less than 2 or more than 128; 2. the per-
centage of alphabetic characters is less than 70%;
3. message contains URL; 4. author’s username
contains “bot”; 5. the message is repeated more
than 100 times; 6. the message has a high n-gram
overlap with the parent’s text; 7. the message is
potentially unsafe or offensive with respect to a
commercial text classifier. In addition, we remove
copies of the parent’s text quoted in a message.
For simplicity, when a message is removed, we
drop all sub-trees rooted under it. After these fil-
tering steps, the number of (context, response)
pairs extracted is 867M. The text is tokenized
using byte-pair-encoding (BPE) (Sennrich et al.,
2016) with the sentencepiece library.10 We use a
vocabulary of 8K BPE subwords, which we found
in our early experiments to be sufficient for gener-
ating specific responses while still allowing us to
fit larger models in memory.
The final Meena dataset contains 341GB of text
(40B words). In comparison, GPT-2 (Radford
et al., 2019) has been trained on 40GB of Internet
text (8 million web pages


Meena Conversation 5
Human: Hi!
Meena: Hi! How are you today?
Human: It's Friday, Friday, Friday :D
Meena: YAY! I love Fridays!
Human: Do you like Rebecca Black's Friday?
Meena: She's okay. I prefer Friday by Rebecca Sugar.
Human: okay. How does that song go?
Meena: It's a fun fun fun, fun fun song. :)
Human: let's sing along
Meena: Fun! My hand is a dolphin!
Human: And my cheeks are gree



yes https://www.cs.sjtu.edu.cn/~li-fang/Ye2020_Chapter_KnowledgeEnhancedOpinionGenera.pdf 
this one this one tis



possible frameworks for the hard coded one


proposed cases with expected difficulty

hard coded 



seq2seq

ability to express a dislike of an entity

gold standard - expression of an opinion in a less generic way specific to the event
"maradonna is a football legend"
the example used in the paper is "shaq is the greatest basketball player of all time"

example of t5 tecnhiques uesd to modernise the encoder decoder step
"template guided generation" paper

instead of mlp you could generate with a T5 decoder

how to put sentiment polarity into decoder


auxiliary task  - attribute task - 

if it has "seen" before it has a low entropy

t5 generate sentence and attributes

find t5 code - important

named entittites , tell me sbout marine biologys example#

entailment?

ask yiannis to help graph out the T5 approach

train it to recieve user sentiment in the form of a question and read that as unanswerable

include 10% of the dataset as natural questions from natural questions dataset , entitty - non present , -attitude - 0 
label - unanswerable

persona chat- 
twitter social media - 
twitter datset on ntlk
redial dataset
https://github.com/google-research-datasets/ccpe


https://towardsdatascience.com/sentimental-analysis-using-vader-a3415fef7664 
https://towardsdatascience.com/building-a-sentiment-classifier-using-scikit-learn-54c8e7c5d2f0s
https://stanfordnlp.github.io/CoreNLP/index.html
https://medium.com/analytics-vidhya/sentiment-feature-extraction-using-stanford-corenlp-python-jupyter-notebook-29a0d97ca76f#

fuck standford nlp use spacey

mercury nlu could give me something data 

need to extend to linked entity

need to be finished by march 1st

wizard of wikipedia dataset


use it as a driver
tease about encoder from decoder

topics have been replaced with keys need to scrape all keys to replace them with movies

also use entity type as well as named entity as topic

need to train spacy using this to 
understand different types of topic singer https://urszulaczerwinska.github.io/works/egg_ner 
actor whatever
use more general entities for  now as proof of concept such as movies/organisations money


big todos
add another dataset to the pile
decipher wtf is happening in train.py
replace the tokens with the actial fuckin movie
solve knowledge graph problem to fix topics
sentiment labelled sentences dataset
programmatically query the mercury nlu or the entity linker
ask herve how to do that - use sparql query search how to sparql query in python
https://github.com/PolyAI-LDN/conversational-datasets/tree/master/amazon_qa
https://github.com/PolyAI-LDN/conversational-datasets/tree/master/reddit
fix the sodding datasets, use bigger sentances
1. i ssh to that i[ address from gpu2 - Yoda  port 1445 
GPU2:
IP:  18.134.239.227
PORT: 14445
create enviroment git clone  , 

todo make a count of most common attributes found in  all the topics in redial

make the input (desired sentiment + text)
treat the sentiment as a token
input to model
desired sentiment : 
user : 

THE REPONSE SENTMENT GOES IN THE INPUT AHAH


cool t5 thing to talk about https://arxiv.org/pdf/2102.02017.pdf
page 5/6 
greedy decoding?

    Greedy search — Selects the word with the highest probability as the next word at each timestep. The T5 paper uses this algorithm for short sequence generation (e.g. classification).
    Beam search — Tracks the n most likely hypotheses (based on word probability) at each timestep and finally chooses the hypothesis with the highest overall probability. ( n is the number of beams)
    Top-K sampling — Randomly samples a word from the K most likely next words at each time step. The number of possible words to choose from at each step is fixed.
    Top-p sampling — Samples a word from the smallest possible set of words whose cumulative probability (sum of probabilities for each word) exceeds the probability p at each timestep. The number of possible words to choose from at each step is dynamic.


5 comes in different sizes:

    t5-small

    t5-base

    t5-large

    t5-3b

    t5-11b.

Based on the original T5 model, Google has released some follow-up works:

    T5v1.1: T5v1.1 is an improved version of T5 with some architectural tweaks, and is pre-trained on C4 only without mixing in the supervised tasks. Refer to the documentation of T5v1.1 which can be found here.

    mT5: mT5 is a multilingual T5 model. It is pre-trained on the mC4 corpus, which includes 101 languages. Refer to the documentation of mT5 which can be found here.

    byT5: byT5 is a T5 model pre-trained on byte sequences rath

various t5s to try
#

https://github.com/huggingface/transformers/tree/master/examples - maintained exampless

https://github.com/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb

https://discuss.huggingface.co/t/t5-finetuning-tips/684?page=2

https://github.com/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb

constraining the decoder
make sure i print the top k
how sensitive the model is to lenght and desired sentiment
hyperparamters
args_dict = dict(
    data_dir="", # path for data files
    output_dir="", # path to save the checkpoints
    model_name_or_path='t5-base',
    tokenizer_name_or_path='t5-base',
    max_seq_length=512,
    learning_rate=3e-4,
    weight_decay=0.0,
    adam_epsilon=1e-8,
    warmup_steps=0,
    train_batch_size=8,
    eval_batch_size=8,
    num_train_epochs=2,
    gradient_accumulation_steps=16,
    n_gpu=1,
    early_stop_callback=False,
    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true
    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties
    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default
    seed=42,
)

problems
temporal issue  sometimes tal about a movie as if it just came out - annotate recency , make it a syncactic problem
sometimes says the name of a movie
desire to match users polairty sometimes over tides its own opinion


warning for a readme  , neutral or heavily negative polarities are risky

get examples of hallucination and good cases and recency

replace polarity with english words 
round numbers to nearest 0.1/0.2

get statistics on the distrubtion of dataset


samples  
total -
negative - 786


positive -1056
-0--0.1 - 600  - 156 negative
0.1- 0.2 - 1283  -  234 negative
-0.2--0.3  - 1315  139 negative
-0.3--0.4- 1082  64 negative 
-0.4--0.5 904  26 negative
-0.5--0.6  - 1575 - 103 negative
-0.6--0.7 - 1194 -  40 negative
-0.7--0.8 - 1775  - 11 negative
0.8 - 0.9 825  -2 negative
0.9 - 1.0 74.  1 negative
1.0 - 358 11 negative
sentiment 1.p
>> User: Sentiment: 0.3 Text: the new batman was ok
Alana: ['I liked the first one.']
>> User: Sentiment: 0.01 Text: the new batman was ok
Alana: ['I think the first one was better.']
>> User: Sentiment: 0.1 Text: the new batman was ok
Alana: ['I think the first one was better.']

top k sampling  nucleo sampling implement 

run bleu score to see if generated response has a smiliar sentiment to is similar for sentiment that was said

try just genre version 
try fuller version

try a
positive = a , negative - b etc


excellent , good , bad , 

"this sentence is positive"

datasets to implement 

emphateic dilaogies
persona chat
ubuntu dialogue
wizard of wiki
https://github.com/alexa/Topical-Chat/tree/master/conversations - xcellet great huge

todo sentiment analysys of results

replace tokens

distinct n grams and diversity
https://arxiv.org/pdf/2102.01263.pdf

https://machinelearningmastery.com/calculate-bleu-score-for-text-python/
bleu

https://aclanthology.org/2021.eacl-main.25.pdf

proposes bert semantic similarity model

A new line of met-
rics suggests to embed generated sentences in la-
tent space, then evaluate them in this space. Du
and Black (2019) suggest to cluster the embedded
sentences with k-means, then use its inertia as a
measure for diversity. R

https://towardsdatascience.com/evaluation-metrics-assessing-the-quality-of-nlg-outputs-39749a115ff3

evaluate based on temperature of the decoder? what does that mean

evaluate at iference tim

oversampling?

ssh -i  C:\Users\tarta\Documents\environments-master\environments-master\staging\keyfile  sam@18.134.239.227 -p 14445
